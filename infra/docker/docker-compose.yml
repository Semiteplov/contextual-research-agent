x-airflow-common: &airflow-common
  image: apache/airflow:2.10.3
  environment: &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "false"
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: "true"
    AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session"
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "postgresql+psycopg2://${POSTGRES_USER:-airflow}:${POSTGRES_PASSWORD:-airflow}@postgres:5432/${POSTGRES_DB:-airflow}"
    AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "true"

    _PIP_ADDITIONAL_REQUIREMENTS: >-
      apache-airflow-providers-postgres==5.13.1
      apache-airflow-providers-amazon==9.0.0 mlflow==2.15.1
      pydantic-settings==2.6.1 pydantic==2.10.6 psycopg2-binary==2.9.10

    AWS_ACCESS_KEY_ID: "${AWS_ACCESS_KEY_ID}"
    AWS_SECRET_ACCESS_KEY: "${AWS_SECRET_ACCESS_KEY}"
    AWS_DEFAULT_REGION: "${AWS_DEFAULT_REGION:-us-east-1}"
    S3_ENDPOINT_URL: "${S3_ENDPOINT_URL:-}"
    AWS_S3_ADDRESSING_STYLE: "${AWS_S3_ADDRESSING_STYLE:-path}"

    MLFLOW_TRACKING_URI: "${MLFLOW_TRACKING_URI:-http://mlflow:5000}"

    PYTHONPATH: "/opt/airflow/src"

  volumes:
    - ../../dags:/opt/airflow/dags
    - ../../contextual_research_agent:/opt/airflow/src/contextual_research_agent
    - airflow_logs:/opt/airflow/logs

  user: "${AIRFLOW_UID:-50000}:0"
  depends_on:
    postgres:
      condition: service_healthy

services:
  postgres:
    image: postgres:16
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-airflow}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-airflow}
      POSTGRES_DB: ${POSTGRES_DB:-airflow}
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./postgres/init:/docker-entrypoint-initdb.d:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-airflow}"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 10s
    restart: always

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow db migrate
        airflow users create \
          --username "${AIRFLOW_ADMIN_USER:-airflow}" \
          --password "${AIRFLOW_ADMIN_PASSWORD:-airflow}" \
          --firstname Admin --lastname User \
          --role Admin \
          --email admin@example.com || true
    environment:
      <<: *airflow-common-env
      _PIP_ADDITIONAL_REQUIREMENTS: ""
    user: "0:0"

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8080:8080"
    depends_on:
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD-SHELL", "curl --fail http://localhost:8080/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 30s
    restart: always

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    depends_on:
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD-SHELL", "curl --fail http://localhost:8974/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 30s
    restart: always

  minio:
    profiles: ["local"]
    image: quay.io/minio/minio:RELEASE.2025-09-07T16-13-09Z
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minioadmin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-minioadmin}
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl -f http://localhost:9000/minio/health/live || exit 1",
        ]
      interval: 10s
      timeout: 5s
      retries: 10

  minio-mc:
    profiles: ["local"]
    image: quay.io/minio/mc:RELEASE.2025-08-13T08-35-41Z
    depends_on:
      minio:
        condition: service_healthy
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minioadmin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-minioadmin}
      S3_BUCKET: ${S3_BUCKET:-rag-storage}
    restart: on-failure
    entrypoint: >
      /bin/sh -c " mc alias set local http://minio:9000 \"$MINIO_ROOT_USER\"
      \"$MINIO_ROOT_PASSWORD\"; mc mb -p \"local/$S3_BUCKET\" || true; echo
      'MinIO buckets ensured'; "

  mlflow:
    image: ghcr.io/mlflow/mlflow:latest
    environment:
      AWS_ACCESS_KEY_ID: "${AWS_ACCESS_KEY_ID}"
      AWS_SECRET_ACCESS_KEY: "${AWS_SECRET_ACCESS_KEY}"
      AWS_DEFAULT_REGION: "${AWS_DEFAULT_REGION:-us-east-1}"
      MLFLOW_S3_ENDPOINT_URL: "${S3_ENDPOINT_URL:-}"
    command:
      - bash
      - -c
      - |
        pip install -q boto3 psycopg2-binary
        mlflow server --host 0.0.0.0 --port 5000 \
          --backend-store-uri "postgresql://${MLFLOW_DB_USER:-airflow}:${MLFLOW_DB_PASSWORD:-airflow}@postgres:5432/${MLFLOW_DB_NAME:-mlflow}" \
          --artifacts-destination "${MLFLOW_ARTIFACT_ROOT:-s3://${S3_BUCKET}/mlflow}"
    ports:
      - "5050:5000"
    depends_on:
      postgres:
        condition: service_healthy
    restart: always

volumes:
  pgdata:
  airflow_logs:
  minio_data:
